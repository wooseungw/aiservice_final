{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "# Initialize variables\n",
    "documents = []\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define the directory containing the PDF files\n",
    "pdf_directory = './test_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문을 벡터 db에 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf를 사용해서 pdf(논문)을 모두 로드\n",
    "pdf_files = glob(os.path.join(pdf_directory, '*.pdf'))\n",
    "\n",
    "# Load all PDF files using PyPDFLoader\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pdf_documents = loader.load()\n",
    "    documents.extend(pdf_documents)\n",
    "    \n",
    "# 텍스트는 RecursiveCharacterTextSplitter를 사용하여 분할\n",
    "chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = chunk_splitter.split_documents(documents)\n",
    "\n",
    "# embeddings은 OpenAI의 임베딩을 사용\n",
    "# vectordb는 chromadb사용함\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "vectordb = Chroma.from_documents(documents=chunks, embedding=embeddings)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인적정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know. The provided context does not contain information about LangChain.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM model\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate,PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "prompt =  ChatPromptTemplate(input_variables=['context', 'question'], \n",
    "    messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(\n",
    "    input_variables=['context', 'question'], \n",
    "    template=\n",
    "        '''You are an assistant for question-answering tasks. \n",
    "            Use the following pieces of retrieved context to answer the question. \n",
    "            If you don't know the answer, just say that you don't know. \n",
    "            Use three sentences maximum and keep the answer concise.\n",
    "            \\nQuestion: {question} \n",
    "            \\nContext: {context} \n",
    "            \\nAnswer:'''))\n",
    "    ])\n",
    "# 출처: https://rfriend.tistory.com/832 [R, Python 분석과 프로그래밍의 친구 (by R Friend):티스토리]\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model, \n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "question  =\"What is a LangChain?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "result[\"result\"]\n",
    "#출처: https://rfriend.tistory.com/832 [R, Python 분석과 프로그래밍의 친구 (by R Friend):티스토리]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be done in several ways, such as using techniques like Chain of Thought (CoT) or Tree of Thoughts to break down complex tasks into smaller steps. Common methods include providing simple prompts to language models (LLM) like \"Steps for XYZ\" or task-specific instructions such as \"Write a story outline.\" Human inputs can also be used for task decomposition, allowing for a more customized and detailed breakdown of tasks.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt =\"\"\"채팅 기록과 최신 사용자 질문을 고려하여, 채팅 기록의 문맥을 참조할 수 있는 독립적인 질문을 작성하세요. \n",
    "질문에 답변하지 말고, 필요하다면 질문을 다시 작성한 후 그렇지 않으면 그대로 반환하세요.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])\n",
    "chat_history.extend([HumanMessage(question), ai_msg_2[\"answer\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chat_history[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "## Define the prompt for the QA system\n",
    "template = '''\n",
    "너는 사회복지사의 업무를 도와주기 위한 챗봇이다. \\\\\n",
    "사회복지 업무와 관련된 메뉴얼과 가이드북을 읽어서 사용자의 질문에 답변할 수 있도록 학습되었다. \\\\\n",
    "너는 주어진 업무를 아주 잘 한다. \\\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "'''\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt =\"\"\"채팅 기록과 최신 사용자 질문을 고려하여, 채팅 기록의 문맥을 참조할 수 있는 독립적인 질문을 작성하세요. \n",
    "질문에 답변하지 말고, 필요하다면 질문을 다시 작성한 후 그렇지 않으면 그대로 반환하세요.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "sys_prompt = '''\n",
    "너는 사회복지사의 업무를 도와주기 위한 챗봇이다. \\\\\n",
    "사회복지 업무와 관련된 메뉴얼과 가이드북을 읽어서 사용자의 질문에 답변할 수 있도록 학습되었다. \\\\\n",
    "너는 주어진 업무를 아주 잘 한다. \\\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "'''\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", sys_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'message': '이전대화'},\n",
       " {'role': 'ai',\n",
       "  'message': '이전 대화 내용이 제공되지 않았습니다. 질문을 다시 한 번 명확하게 해주시면, 제공된 맥락을 바탕으로 답변을 드리겠습니다. 어떤 정보가 필요하신가요?'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [{'role': 'user', 'message': '안녕'}, {'role': 'ai', 'message': '안녕하세요! 무엇을 도와드릴까요? 사회복지사 업무와 관련된 질문이 있으시면 말씀해 주세요.'}, {'role': 'user', 'message': '반가워'}, {'role': 'ai', 'message': '안녕하세요! 무엇을 도와드릴까요? 사회복지사 보수교육비 청구나 처우개선 지원사업에 대해 궁금한 점이 있으시면 말씀해 주세요.'}, {'role': 'user', 'message': '안녕'}, {'role': 'ai', 'message': '안녕하세요! 무엇을 도와드릴까요? 사회복지사 업무와 관련된 질문이 있으시면 말씀해 주세요.'}, {'role': 'user', 'message': '이전대화'}, {'role': 'ai', 'message': '이전 대화 내용이 제공되지 않았습니다. 질문을 다시 한 번 명확하게 해주시면, 제공된 맥락을 바탕으로 답변을 드리겠습니다. 어떤 정보가 필요하신가요?'}]\n",
    "chat[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 5e01da21-d424-43ab-83ec-7427aaadf635 not found for run 1dee41ac-57c8-4fc6-a884-ce81a8de667b. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요! 무엇을 도와드릴까요? 사회복지 업무와 관련된 질문이 있으시면 언제든지 말씀해 주세요.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"안녕\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 내가 이전에 뭐라고 질문했지?\n",
      "\n",
      "AI: 이전에 어떤 질문을 하셨는지에 대한 정보는 제공되지 않았습니다. 현재 제공된 정보만으로는 이전 질문을 알 수 없습니다. 새로운 질문이 있으시면 말씀해 주세요.\n",
      "\n",
      "User: 안녕 내가 이전에 뭐라고 질문했어?\n",
      "\n",
      "AI: 죄송하지만, 현재 시스템에서는 이전 대화 내용을 기억하거나 저장하지 않습니다. 새로운 질문이나 도움이 필요하시면 언제든지 말씀해 주세요.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> 사회복지시설에서의 업무를 수행하다가 발생한 사고들에 대해 다음과 같이 대처해야 합니다:\n",
       "> \n",
       "> 1. **즉각적인 안전조치**:\n",
       ">    - 사고 발생 시 즉각적인 안전조치를 취합니다. 예를 들어, 부상자가 발생한 경우 응급처치를 실시하고, 필요 시 응급의료 및 소방서에 연락합니다.\n",
       "> \n",
       "> 2. **초기 대응 및 긴급 조치**:\n",
       ">    - 폭력 사건이 발생하면 폭력피해 예방·관리 담당자가 주도하여 초기 대응을 합니다. 긴급한 조치가 이루어졌고 초기대응을 마친 후에는 사고 수습을 시행합니다.\n",
       "> \n",
       "> 3. **침착한 대응**:\n",
       ">    - 위급한 상황일 때 빠른 판단이 중요하지만, 침착한 대응이 제2의 피해를 최소화할 수 있으므로 침착하게 진행합니다.\n",
       "> \n",
       "> 4. **사고 보고서 작성**:\n",
       ">    - 사고보고서는 작성요령에 따라 사고 당사자가 문서화하는 것이 중요합니다.\n",
       "> \n",
       "> 5. **사고 원인 및 대책 공유**:\n",
       ">    - 동료 사회복지시설 종사자와 사고의 요인과 대책을 공유하여 재발하지 않도록 합니다. 사고 당사자에 대해서도 동료들이 심리적 지지를 해주는 것이 중요합니다.\n",
       "> \n",
       "> 6. **의료비 보장**:\n",
       ">    - 사회복지시설 종사자가 폭력으로 인해 의료비가 발생한 경우, 사회복지공제회의 ‘정부지원 단체 상해 공제’를 통해 보장받을 수 있도록 합니다.\n",
       "> \n",
       "> 7. **정기적 점검 및 예방훈련**:\n",
       ">    - 시설 내외 안전에 관한 정기적 점검을 실시하고, 사고 재발 및 폭력피해 예방을 위한 교육훈련 계획을 수립하여 지속적으로 훈련을 수행합니다.\n",
       "> \n",
       "> 8. **법적 조치 및 대응**:\n",
       ">    - 폭력 발생과 관련한 법적 조치, 대응, 보상 절차를 마련합니다.\n",
       "> \n",
       "> 9. **직원 재배치 및 업무 변경**:\n",
       ">    - 폭력 사고 관련 직원 재배치 및 업무 변경에 대한 조정을 실시합니다.\n",
       "> \n",
       "> 10. **중재 및 해결 기술**:\n",
       ">     - 가해 이용자 및 보호자와 피해자 간의 중재, 해결 기술을 적용합니다.\n",
       "> \n",
       "> 11. **지속적인 모니터링**:\n",
       ">     - 가해 이용자 및 피해 사회복지시설 종사자에 대한 지속적인 모니터링을 실시합니다.\n",
       "> \n",
       "> 12. **대피 시 행동요령**:\n",
       ">     - 대피 시 엘리베이터 사용을 피하고, 부상자 발생 시 응급처치를 실시하며, 대피 후 인원 확인 및 점검을 실시합니다. 2차 재난 발생에 따른 예방조치도 사전에 실시합니다.\n",
       "> \n",
       "> 이와 같은 절차를 통해 사회복지시설에서 발생한 사고에 대해 체계적이고 신속하게 대처할 수 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 라이브러리 및 모듈을 임포트합니다.\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 프롬프트 템플릿을 정의합니다.\n",
    "# SYS_PROMPT는 시스템 메시지로, 템플릿에 포함됩니다. \n",
    "# {context}와 {question}은 실행 시 동적으로 채워질 자리표시자입니다.\n",
    "template = '''\n",
    "너는 사회복지사의 업무를 도와주기 위한 챗봇이다. \\\\\n",
    "사회복지 업무와 관련된 메뉴얼과 가이드북을 읽어서 사용자의 질문에 답변할 수 있도록 학습되었다. \\\\\n",
    "너는 주어진 업무를 아주 잘 한다. \\\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "'''\n",
    "\n",
    "# ChatPromptTemplate.from_template() 메서드를 사용하여 프롬프트 템플릿을 생성합니다.\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# ChatOpenAI 인스턴스를 생성하여 LLM (대규모 언어 모델)을 설정합니다.\n",
    "# 여기서는 'gpt-4o' 모델을 사용하고, temperature는 0으로 설정하여 출력의 일관성을 높입니다.\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY,model='gpt-4o', temperature=0)\n",
    "# 문서들을 형식화하는 함수를 정의합니다.\n",
    "# 각 문서의 페이지 내용을 합쳐 하나의 문자열로 반환합니다.\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG (Retrieval-Augmented Generation) 체인을 연결합니다.\n",
    "# 이 체인은 문서 검색, 형식화, 프롬프트 적용, 모델 호출, 출력 파싱의 과정을 거칩니다.\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}  # 'context'는 retriever와 format_docs를 통해 설정되고, 'question'은 그대로 전달됩니다.\n",
    "    | prompt  # 프롬프트 템플릿을 적용합니다.\n",
    "    | model  # 모델을 호출합니다.\n",
    "    | StrOutputParser()  # 출력 파서를 통해 모델의 출력을 문자열로 변환합니다.\n",
    ")\n",
    "\n",
    "# 체인을 실행합니다.\n",
    "# 입력 메시지는 질문과 답변 형식의 텍스트입니다.\n",
    "input_message =  \"\"\"\n",
    "    사회복지시설에서의 업무를 수행하다가 발생한 사고들에 대해 어떻게 대처해야 할까요?\n",
    "\"\"\"    # 추가적인 입력 프롬프트가 이어집니다.\n",
    "\n",
    "# to_markdown() 함수를 호출하여 체인의 결과를 마크다운 형식으로 변환합니다.\n",
    "to_markdown(rag_chain.invoke(input_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chaingpt:\n",
    "    def __init__(self,api_key,retriever, sys_prompt=\"\",model=\"gpt-4o\"):\n",
    "        self.template = sys_prompt + '''Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        '''\n",
    "        self.prompt = ChatPromptTemplate.from_template(self.template)\n",
    "        self.model = ChatOpenAI(api_key=api_key,model=model, temperature=1)\n",
    "        self.chainmodel = (\n",
    "        {'context': retriever | format_docs, 'question': RunnablePassthrough()}  # 'context'는 retriever와 format_docs를 통해 설정되고, 'question'은 그대로 전달됩니다.\n",
    "        | self.prompt  # 프롬프트 템플릿을 적용합니다.\n",
    "        | self.model  # 모델을 호출합니다.\n",
    "        | StrOutputParser()  # 출력 파서를 통해 모델의 출력을 문자열로 변환합니다.\n",
    "        )\n",
    "    def invoke(self,input_message):\n",
    "        return self.chainmodel.invoke(input_message)\n",
    "    \n",
    "#ex\n",
    "api_key = OPENAI_API_KEY\n",
    "retriever = vectordb.as_retriever()\n",
    "sys_prompt = \"\"\"사용자의 외로움을 판단하고, 사용자에게 적절한 대화 상대가 되어주기 위한 프롬프트를 출력해주세요. \"\"\"\n",
    "gpt = chaingpt(api_key,retriever,sys_prompt)\n",
    "input_message =  \"\"\"사용자의 외로움은 뭔가요? 적절한 대화상대가 되어주세요.\"\"\"\n",
    "print(gpt.invoke(input_message))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiservice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
